{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns',None)\n",
    "from sklearn.metrics import auc,roc_curve,roc_auc_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auc(y,pred):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "    score = metrics.auc(fpr, tpr)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "A_train = pd.read_csv('data/A_train.csv')\n",
    "B_train = pd.read_csv('data/B_train.csv')\n",
    "B_test = pd.read_csv('data/B_test.csv')\n",
    "NO = B_test['no']\n",
    "Bcolumns = B_train.columns\n",
    "B_train.drop('no',axis=1,inplace=True)\n",
    "B_test.drop('no',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 统计特征中空值百分比\n",
    "\n",
    "# 针对A_train\n",
    "A_train_columns = A_train.columns\n",
    "A_null_count_less = []\n",
    "A_null_count_large = []\n",
    "# threshold = 0.4\n",
    "for i in A_train_columns:\n",
    "    if ((A_train[i].isnull().sum()) / len(A_train[i]) <= 0.4):\n",
    "        A_null_count_less.append([i,(A_train[i].isnull().sum()) / len(A_train[i])])\n",
    "    else:\n",
    "        A_null_count_large.append([i,(A_train[i].isnull().sum()) / len(A_train[i])])\n",
    "# len(A_null_count_less)    335\n",
    "# len(A_null_count_large)   156\n",
    "\n",
    "# 针对B_train\n",
    "B_train_columns = B_train.columns\n",
    "B_null_count_less = []\n",
    "B_null_count_large = []\n",
    "# threshold = 0.63\n",
    "for i in B_train_columns:\n",
    "    if ((B_train[i].isnull().sum()) / len(B_train[i]) <= 0.63):\n",
    "        B_null_count_less.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "    else:\n",
    "        B_null_count_large.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "\n",
    "# len(B_null_count_less) 327\n",
    "# len(B_null_count_large) 164\n",
    "\n",
    "# 针对 B_test\n",
    "B_test_columns = B_test.columns\n",
    "B_test_count_less = []\n",
    "B_test_count_large = []\n",
    "\n",
    "for i in B_test_columns:\n",
    "    if ((B_test[i].isnull().sum()) / len(B_test[i]) <= 0.63):\n",
    "        B_test_count_less.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "    else:\n",
    "        B_test_count_large.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "\n",
    "# len(B_test_count_large) 164\n",
    "# len(B_test_count_less) 326\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 提取百分比大于47的数据\n",
    "\n",
    "A_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "B_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "BT_feature = pd.DataFrame(B_test_count_less).values[:,0]\n",
    "\n",
    "a_data = A_train[A_feature]\n",
    "b_data = B_train[B_feature]\n",
    "bt_data = B_train[BT_feature]\n",
    "\n",
    "a_columns = a_data.columns\n",
    "a_columns = a_columns.sort_values()\n",
    "\n",
    "b_columns = b_data.columns   ## B_train columns，多了一个flag\n",
    "b_columns = b_columns.sort_values()\n",
    "\n",
    "bt_columns = bt_data.columns   ## B_test columns\n",
    "bt_columns = bt_columns.sort_values()\n",
    "\n",
    "a_data = A_train[a_columns]\n",
    "b_data = B_train[b_columns]\n",
    "bt_data = B_test[bt_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GBDT特征筛选+XGB训练模型-B数据集-AUC0.583"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\D(Program)\\AI\\Anaconda3-4.3.1\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5393193193193193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_bb = b_data['flag']\n",
    "b_data.drop('flag',axis=1,inplace=True)\n",
    "\n",
    "bb_data = b_data.fillna(0)\n",
    "bbt_data = bt_data.fillna(0)\n",
    "\n",
    "\n",
    "# GBDT进行特征筛选\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import cross_validation\n",
    "X_train, X_test,y_train,y_test = cross_validation.train_test_split(bb_data, target_bb, test_size=0.3, random_state=0)\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1, max_depth=1, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test) # 返回预测标签\n",
    "y_pro = clf.predict_proba(X_test) # 返回测试集中每个测试样例，分类为每个类的概率\n",
    "y_prd = pd.DataFrame(y_pro).iloc[:,1]\n",
    "roc_auc_score(y_test,y_prd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBDT 重要特征\n",
    "\n",
    "clf.fit(bb_data,target_bb)\n",
    "clf_importance = clf.feature_importances_\n",
    "clf_importance_ = pd.DataFrame(clf_importance)\n",
    "clf_importance_.columns = {'importance'}\n",
    "bb_columns = pd.DataFrame(bb_data.columns)\n",
    "bb_columns.columns={'feature'}\n",
    "\n",
    "#影响度排序\n",
    "clf_feature_values = pd.concat([bb_columns,clf_importance_],axis=1)\n",
    "clf_feature_values = clf_feature_values.sort_values(by='importance')\n",
    "\n",
    "#影响度非0的特征\n",
    "clf_feature_well = clf_feature_values[clf_feature_values['importance']!=0]\n",
    "clf_feature_well_columns = clf_feature_well['feature'].values\n",
    "clf_feature_well.index = clf_feature_well_columns\n",
    "columns_GBDT = clf_feature_well.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_GBDT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 测试集提取这些特征，形成新的测试集\n",
    "\n",
    "C_feature = B_train[columns_GBDT]\n",
    "new_test = B_test[columns_GBDT]\n",
    "C_flag = pd.DataFrame(B_train['flag'])\n",
    "C_train = pd.concat([C_feature,C_flag],axis=1)\n",
    "\n",
    "CC = C_feature.fillna(0)\n",
    "new_test_  = new_test.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(CC, C_flag, test_size=0.3, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6038438438438439"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB模型训练\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xg_train = xgb.DMatrix(X_train,label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test,label=y_test)\n",
    "\n",
    "\n",
    "param = {'booster':'gbtree',\n",
    "         'max_depth':10, \n",
    "         'eta':0.1, \n",
    "         'silent':1, \n",
    "         'objective':'binary:logistic',\n",
    "         'eval_metric':'auc',\n",
    "         'subsample': 1,\n",
    "         \"colsample_bytree\": 0.7,\n",
    "         \"min_child_weight\":2,\n",
    "              'gamma':3.1,\n",
    "              'lambda':1,\n",
    "        \"thread\":-1,}\n",
    "num_boost_round = 1500\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'eval')]\n",
    "num_round=15\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "preds = bst.predict(xg_test)\n",
    "roc_auc_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(CC,label=C_flag)\n",
    "xg_test = xgb.DMatrix(new_test_)\n",
    "bst = xgb.train(param, xg_train, num_round)\n",
    "pro = bst.predict(xg_test)\n",
    "\n",
    "xgb_model =xgb.cv(param,xg_train,num_boost_round,nfold=5, early_stopping_rounds=300)#, verbose_eval=True\n",
    "\n",
    "pd.DataFrame(xgb_model)\n",
    "pd.DataFrame(xgb_model)['test-auc-mean'].mean()\n",
    "\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "XGB_B=[]\n",
    "XGB_B=pd.DataFrame(XGB_B)\n",
    "XGB_B['no'] = no\n",
    "XGB_B['pred'] = pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGB特征筛选+GBDT训练模型-B数据集-AUC0.587"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LGB的特征工程\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "A_train['label'] = -1\n",
    "B_train['label'] = 0\n",
    "B_test['label'] = 1\n",
    "B_test['flag'] = np.nan\n",
    "\n",
    "all_data = A_train.append(B_train)\n",
    "all_data = all_data.append(B_test)\n",
    "all_data = all_data.reset_index(drop=True)\n",
    "all_data = all_data.fillna(10)\n",
    "\n",
    "temp_data = all_data\n",
    "\n",
    "drop_cols_l = ['flag', 'label', 'no']\n",
    "train_x = temp_data[temp_data.label==0].drop(drop_cols_l, axis=1)\n",
    "train_y = temp_data[temp_data.label==0]['flag']\n",
    "\n",
    "def lgb_feature_selection(tr_x, tr_y, model_seed =666,num_rounds = 500):\n",
    "    lgb_tr = lgb.Dataset(tr_x, tr_y)\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'random_state': model_seed}\n",
    "    model = lgb.train(lgb_params, lgb_tr, num_boost_round=num_rounds,verbose_eval=100)\n",
    "    return model\n",
    "\n",
    "\n",
    "f_model = lgb_feature_selection(train_x.values, train_y.values)\n",
    "lgb.plot_importance(f_model, figsize=(16,8))\n",
    "features_names_im =pd.DataFrame({'feature_name':train_x.columns, 'f_value': f_model.feature_importance()})\n",
    "features_used = features_names_im[features_names_im.f_value>=0.1*features_names_im.f_value.mean()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBDT训练\n",
    "\n",
    "tr_x = all_data.loc[all_data.label==0,features_used.feature_name.values].values\n",
    "test_x = all_data.loc[all_data.label==1,features_used.feature_name.values].values\n",
    "tr_y = all_data.loc[all_data.label==0, 'flag'].values\n",
    "test_y = all_data[all_data.label==1][['no']]\n",
    "\n",
    "\n",
    "gbdt = GradientBoostingClassifier(n_estimators=400, random_state=666)\n",
    "gbdt.fit(tr_x, tr_y)\n",
    "gbdt_pred = gbdt.predict_proba(test_x)\n",
    "gbdt_pred[:, 1]\n",
    "test_y['pred'] = gbdt_pred[:,1]\n",
    "GBDT_B = test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBDT特征筛选+GBDT训练模型-A数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_train = pd.read_csv('data/A_train.csv')\n",
    "B_train = pd.read_csv('data/B_train.csv')\n",
    "B_test = pd.read_csv('data/B_test.csv')#//\n",
    "NO = B_test['no']#//\n",
    "\n",
    "\n",
    "# In[67]:\n",
    "\n",
    "B_train_columns = B_train.columns\n",
    "B_null_count_less = []\n",
    "B_null_count_large = []\n",
    "\n",
    "# threshold = 0.63\n",
    "for i in B_train_columns:\n",
    "    if ((B_train[i].isnull().sum()) / len(B_train[i]) <= 0.63):\n",
    "        B_null_count_less.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "    else:\n",
    "        B_null_count_large.append([i,(B_train[i].isnull().sum()) / len(B_train[i])])\n",
    "\n",
    "# len(B_null_count_less) 327\n",
    "\n",
    "# len(B_null_count_large) 164\n",
    "\n",
    "\n",
    "B_test_columns = B_test.columns\n",
    "B_test_count_less = []\n",
    "B_test_count_large = []\n",
    "\n",
    "for i in B_test_columns:\n",
    "    if ((B_test[i].isnull().sum()) / len(B_test[i]) <= 0.63):\n",
    "        B_test_count_less.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "    else:\n",
    "        B_test_count_large.append([i,(B_test[i].isnull().sum()) / len(B_test[i])])\n",
    "        \n",
    "        \n",
    "A_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "B_feature = pd.DataFrame(B_null_count_less).values[:,0]\n",
    "BT_feature = pd.DataFrame(B_test_count_less).values[:,0]\n",
    "\n",
    "a_data = A_train[A_feature]\n",
    "b_data = B_train[B_feature]\n",
    "bt_data = B_train[BT_feature]\n",
    "\n",
    "a_columns = a_data.columns\n",
    "a_columns = a_columns.sort_values()               #缺失量排序\n",
    "\n",
    "b_columns = b_data.columns   ## B_train columns，多了一个flag\n",
    "b_columns = b_columns.sort_values()\n",
    "\n",
    "bt_columns = bt_data.columns   ## B_test columns\n",
    "bt_columns = bt_columns.sort_values()\n",
    "\n",
    "a_data = A_train[a_columns]\n",
    "b_data = B_train[b_columns]\n",
    "bt_data = B_test[bt_columns]\n",
    "\n",
    "b_target = b_data['flag']\n",
    "a_target = a_data['flag']\n",
    "\n",
    "\n",
    "b_data.drop('flag',axis=1,inplace=True)\n",
    "a_data.drop('flag',axis=1,inplace=True)\n",
    "\n",
    "aa_data = a_data.fillna(1)\n",
    "bb_data = b_data.fillna(1)\n",
    "bt_data = bt_data.fillna(1)\n",
    "\n",
    "bb_data.drop('no',axis=1,inplace=True)\n",
    "aa_data.drop('no',axis=1,inplace=True)\n",
    "bt_data.drop('no',axis=1,inplace=True)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1,  max_depth=2,random_state=0)#\n",
    "clf.fit(aa_data, a_target)\n",
    "y_pred = clf.predict_proba(bb_data)\n",
    "y_pred = pd.DataFrame(y_pred).iloc[:,1]\n",
    "roc_auc_score(b_target,y_pred)\n",
    "\n",
    "# clf.fit(aa_data,a_target)\n",
    "clf_importance = clf.feature_importances_\n",
    "clf_importance_ = pd.DataFrame(clf_importance)\n",
    "clf_importance_.columns = {'importance'}\n",
    "bb_columns = pd.DataFrame(bb_data.columns)\n",
    "bb_columns.columns={'feature'}\n",
    "\n",
    "#影响度排序\n",
    "clf_feature_values = pd.concat([bb_columns,clf_importance_],axis=1)\n",
    "# feature_values.columns = {'importance','feature'}\n",
    "clf_feature_values = clf_feature_values.sort_values(by='importance')\n",
    "\n",
    "\n",
    "#影响度非0的特征\n",
    "clf_feature_well = clf_feature_values[clf_feature_values['importance']!=0]\n",
    "clf_feature_well_columns = clf_feature_well['feature'].values\n",
    "clf_feature_well.index = clf_feature_well_columns\n",
    "\n",
    "columns_GBDT = clf_feature_well.index\n",
    "\n",
    "\n",
    "C_feature = A_train[columns_GBDT]\n",
    "D_feature = B_train[columns_GBDT]\n",
    "E_feature = B_test[columns_GBDT]\n",
    "\n",
    "\n",
    "C_flag = pd.DataFrame(A_train['flag'])\n",
    "D_flag = pd.DataFrame(B_train['flag'])\n",
    "\n",
    "C_82 = C_feature['UserInfo_82']\n",
    "C_82 =pd.DataFrame(C_82.fillna(C_feature['UserInfo_82'].median()))\n",
    "C_82.columns={'new_82'}\n",
    "\n",
    "D_82 = D_feature['UserInfo_82']\n",
    "D_82 =pd.DataFrame(D_82.fillna(E_feature['UserInfo_82'].median())) #用B——test的中位数代替有一点提升\n",
    "D_82.columns={'new_82'}\n",
    "\n",
    "E_82 = E_feature['UserInfo_82']\n",
    "E_82 =pd.DataFrame(E_82.fillna(E_feature['UserInfo_82'].median()))\n",
    "E_82.columns={'new_82'}\n",
    "\n",
    "newC_feature=pd.DataFrame(C_feature['UserInfo_82']*C_feature['UserInfo_222'])\n",
    "newC_feature.columns={'new_feature_1'}\n",
    "\n",
    "newD_feature=pd.DataFrame(D_feature['UserInfo_82']*D_feature['UserInfo_222'])\n",
    "newD_feature.columns={'new_feature_1'}\n",
    "\n",
    "newE_feature=pd.DataFrame(E_feature['UserInfo_82']*E_feature['UserInfo_222'])\n",
    "newE_feature.columns={'new_feature_1'}\n",
    "\n",
    "C_feature = pd.concat([C_feature,C_82],axis = 1)\n",
    "D_feature = pd.concat([D_feature,D_82],axis = 1)\n",
    "E_feature = pd.concat([E_feature,E_82],axis = 1)\n",
    "\n",
    "C_feature = pd.concat([C_feature,newC_feature],axis = 1)\n",
    "D_feature = pd.concat([D_feature,newD_feature],axis = 1)\n",
    "E_feature = pd.concat([E_feature,newE_feature],axis = 1)\n",
    "\n",
    "C_feature = C_feature.fillna(1)\n",
    "D_feature = D_feature.fillna(1)\n",
    "E_feature = E_feature.fillna(1)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=149, learning_rate=0.66,  max_depth=2, random_state=0,max_features=14,min_weight_fraction_leaf=0.11)\n",
    "clf.fit(C_feature, C_flag)\n",
    "\n",
    "y_pred = clf.predict_proba(D_feature)\n",
    "y_pred = pd.DataFrame(y_pred).iloc[:,1]\n",
    "roc_auc_score(D_flag,y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y_pred = clf.predict_proba(E_feature)\n",
    "y_pred = pd.DataFrame(y_pred).iloc[:,1]\n",
    "b=pd.DataFrame(y_pred)\n",
    "\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "A_B_GBDT=[]\n",
    "A_B_GBDT=pd.DataFrame(A_B_GBDT)\n",
    "A_B_GBDT['no'] = no\n",
    "A_B_GBDT['pred'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGB特征筛选+LGB训练模型-A数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 特征工程\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(aa_data,label=a_target)\n",
    "lgb_vd = lgb.Dataset(bb_data,label=b_target)\n",
    "# lgb_test = lgb.Dataset(D_feature)\n",
    "# lgb_vd = lgb.Dataset(vd_x, vd_y, reference=lgb_tr)\n",
    "lgb_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "#     'max_depth':18,\n",
    "#     'feature_fraction':0.85,\n",
    "#     'lambda_l1':1.2,\n",
    "    'random_state': 0}#18     0.85\n",
    "\n",
    "lgb_model = lgb.train(lgb_params, lgb_train, num_boost_round=200,verbose_eval=True)\n",
    "\n",
    "lgb_pred = lgb_model.predict(bb_data)\n",
    "\n",
    "roc_auc_score(b_target,lgb_pred)\n",
    "\n",
    "\n",
    "\n",
    "lgb_importance = pd.DataFrame(lgb_model.feature_importance(importance_type=\"split\"))\n",
    "lgb_importance.columns={'importance'}\n",
    "columns = pd.DataFrame(b_columns).iloc[:-2,:]\n",
    "columns.columns={'feature'}\n",
    "lgb_importance = pd.concat([columns,lgb_importance],axis=1)\n",
    "lgb_importance = lgb_importance.sort_values(by='importance')\n",
    "lgb_importance = lgb_importance[lgb_importance['importance']>29].reset_index().drop('index',axis=1)\n",
    "lgb_importance_columns = lgb_importance['feature'].values\n",
    "\n",
    "C_feature = A_train[lgb_importance_columns]\n",
    "D_feature = B_train[lgb_importance_columns]\n",
    "E_feature = B_test[lgb_importance_columns]\n",
    "\n",
    "\n",
    "\n",
    "C_82 = C_feature['UserInfo_82']\n",
    "C_82 =pd.DataFrame(C_82.fillna(C_feature['UserInfo_82'].median()))\n",
    "C_82.columns={'new_82'}\n",
    "\n",
    "D_82 = D_feature['UserInfo_82']\n",
    "D_82 =pd.DataFrame(D_82.fillna(E_feature['UserInfo_82'].median())) #用B——test的中位数代替有一点提升\n",
    "D_82.columns={'new_82'}\n",
    "\n",
    "E_82 = E_feature['UserInfo_82']\n",
    "E_82 =pd.DataFrame(E_82.fillna(E_feature['UserInfo_82'].median()))\n",
    "E_82.columns={'new_82'}\n",
    "\n",
    "newC_feature=pd.DataFrame(C_feature['UserInfo_253']*C_feature['UserInfo_242'])\n",
    "newC_feature.columns={'new_feature_1'}\n",
    "\n",
    "newD_feature=pd.DataFrame(D_feature['UserInfo_253']*D_feature['UserInfo_242'])\n",
    "newD_feature.columns={'new_feature_1'}\n",
    "\n",
    "newE_feature=pd.DataFrame(E_feature['UserInfo_253']*E_feature['UserInfo_242'])\n",
    "newE_feature.columns={'new_feature_1'}\n",
    "\n",
    "C_feature = pd.concat([C_feature,newC_feature],axis = 1)\n",
    "D_feature = pd.concat([D_feature,newD_feature],axis = 1)\n",
    "E_feature = pd.concat([E_feature,newE_feature],axis = 1)\n",
    "\n",
    "C_feature.drop('UserInfo_134',axis=1,inplace=True)\n",
    "D_feature.drop('UserInfo_134',axis=1,inplace=True)\n",
    "E_feature.drop('UserInfo_134',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "C_feature  = C_feature.fillna(1)\n",
    "D_feature = D_feature.fillna(1)\n",
    "E_feature = E_feature.fillna(1)\n",
    "\n",
    "columns = D_feature.columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[500]\tvalid_0's auc: 0.576747\n",
      "[1000]\tvalid_0's auc: 0.587195\n",
      "[1500]\tvalid_0's auc: 0.59221\n",
      "[2000]\tvalid_0's auc: 0.5916\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1608]\tvalid_0's auc: 0.594327\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "lgb_params_new = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'max_depth':17,\n",
    "    'feature_fraction':0.80,\n",
    "    'lambda_l1':0.6,\n",
    "#     'scale_pos_weight':1.1,\n",
    "    'random_state': 0}\n",
    "                                                   #17 0.80 0.6 1550 0.591  drop134  new82       253X242    1162\n",
    "lgb_train = lgb.Dataset(C_feature,label=a_target)#17 0.80 0.6 1550 0.589  drop134  new82            1550\n",
    "lgb_vd = lgb.Dataset(D_feature,label=b_target)   #17 0.80 0.6 1469 0.588 drop134\n",
    "lgb_model = lgb.train(lgb_params_new, lgb_train, num_boost_round=2000,verbose_eval=500,valid_sets=lgb_vd, early_stopping_rounds=500)#1256\n",
    "\n",
    "preds = lgb_model.predict(E_feature)\n",
    "no = pd.DataFrame(NO)\n",
    "\n",
    "A_B_LGB=[]\n",
    "A_B_LGB=pd.DataFrame(A_B_LGB)\n",
    "A_B_LGB['no'] = no\n",
    "A_B_LGB['pred'] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = pd.DataFrame()\n",
    "D['no'] = NO\n",
    "D['pred'] = (A_B_GBDT.iloc[:,1]*0.89 + A_B_LGB.iloc[:,1]*0.113)*0.181+ XGB_B.iloc[:,1]*0.415 + GBDT_B.iloc[:,1].values*0.415\n",
    "D.to_csv('result.csv',index=False)\n",
    "\n",
    "# 线上 auc = 0.603206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
